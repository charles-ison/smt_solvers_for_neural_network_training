{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "64dd2c65-4e37-4dc8-b93e-0bee5546c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pysmt.shortcuts import Symbol, And, Equals, Real, GT, Max, Plus, get_model\n",
    "from pysmt.typing import REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "663340c8-ca5f-45f9-aa03-80bb4c7483d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(Model, self).__init__()             \n",
    "        self.linear0 = nn.Linear(3, embedding_size)\n",
    "        self.linear1 = nn.Linear(embedding_size, 1)\n",
    "        self.linear_layers = [self.linear0, self.linear1]\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):   \n",
    "        x = self.linear0(x)\n",
    "        x = self.relu(x)  \n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x) \n",
    "        \n",
    "        return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f56893e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_backpropagation(model, all_data, labels, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        num_correct = 0\n",
    "    \n",
    "        for (data, label) in zip(all_data, labels): \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            output = model(data)\n",
    "            prediction = 1 if output > 0 else 0\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "            num_correct += (label == prediction)\n",
    "        \n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss = running_loss/len(labels)\n",
    "        accuracy = num_correct/len(labels)\n",
    "        print(\"epoch: \" + str(epoch) + \", accuracy: \" + str(accuracy.item()) + \", loss: \" + str(loss))\n",
    "        \n",
    "        if (accuracy == 1.0):\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "460c9e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_smt(model, all_data, labels):\n",
    "    \n",
    "    layers_weights = []\n",
    "    num_layers = len(model.linear_layers)\n",
    "    for i in range(num_layers):\n",
    "        num_rows = len(model.linear_layers[i].weight.data)\n",
    "        num_cols = len(model.linear_layers[i].weight.data[0])\n",
    "        layer_weights = []\n",
    "        for j in range(num_rows):\n",
    "            weights = []\n",
    "            for k in range(num_cols):\n",
    "                weights.append(Symbol(\"weight_\" + str(i) + \"_\" + str(j) + \"_\" + str(k), REAL))\n",
    "            weights.append(Symbol(str(\"bias_\" + str(i) + \"_\" + str(j)), REAL))\n",
    "            layer_weights.append(weights)\n",
    "        layers_weights.append(layer_weights)\n",
    "\n",
    "    equations = []\n",
    "    for (data, label) in zip(all_data, labels):\n",
    "        x = []\n",
    "        for value in data:\n",
    "            x.append(Real(value.item()))\n",
    "            \n",
    "        for i in range(num_layers):\n",
    "            num_rows = len(model.linear_layers[i].weight.data)\n",
    "            num_cols = len(model.linear_layers[i].weight.data[0])\n",
    "            new_x = []\n",
    "            for j in range(num_rows):\n",
    "                calculations = []\n",
    "                for k in range(num_cols):\n",
    "                    calculation = layers_weights[i][j][k] * x[k]\n",
    "                    calculations.append(calculation)\n",
    "                calculations.append(layers_weights[i][j][num_cols])\n",
    "                calculation = Plus(calculations)\n",
    "                relu = Max(calculation, Real(0.0))\n",
    "                new_x.append(relu)\n",
    "            x = new_x\n",
    "    \n",
    "        if label.item() == 0.0:\n",
    "            equation = Equals(x[0], Real(0.0))\n",
    "        else:\n",
    "            equation = GT(x[0], Real(0.0))\n",
    "            \n",
    "        equations.append(equation)\n",
    "\n",
    "    formula = And(equations)\n",
    "    solution = get_model(formula)\n",
    "    \n",
    "    if solution:\n",
    "        for i in range(num_layers):\n",
    "            num_rows = len(model.linear_layers[i].weight.data)\n",
    "            num_cols = len(model.linear_layers[i].weight.data[0])\n",
    "            for j in range(num_rows):\n",
    "                for k in range(num_cols):\n",
    "                    weight = layers_weights[i][j][k]\n",
    "                    weight_solution = solution[weight].constant_value()\n",
    "                    model.linear_layers[i].weight.data[j][k] = float(weight_solution)\n",
    "    \n",
    "                \n",
    "                bias = layers_weights[i][j][num_cols]\n",
    "                bias_solution = solution[bias].constant_value()\n",
    "                model.linear_layers[i].bias.data[j] = float(bias_solution)\n",
    "\n",
    "    else:\n",
    "        print(\"No solution found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b2a498e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, all_data, labels, criterion): \n",
    "    running_loss = 0.0\n",
    "    num_correct = 0\n",
    "    \n",
    "    for (data, label) in zip(all_data, labels): \n",
    "        output = model(data)\n",
    "        prediction = 1 if output > 0 else 0\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        num_correct += (label == prediction)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    loss = running_loss/len(labels)\n",
    "    accuracy = num_correct/len(labels)\n",
    "    \n",
    "    print(\"testing accuracy: \" + str(accuracy.item()) + \" and loss: \" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3851b7bc-a254-4ce2-ac88-bbe5bfdce954",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "points = torch.tensor([[-1.0, -1.0, -1.0], [1.0, -1.0, -1.0], [-1.0, 1.0, -1.0], [-1.0, -1.0, 1.0],\n",
    "                       [1.0, 1.0, 1.0], [-1.0, 1.0, 1.0], [1.0, -1.0, 1.0], [1.0, 1.0, -1.0],])\n",
    "\n",
    "labels = torch.tensor([0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0], requires_grad=True)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4abcfc74-b773-4876-a915-a11a28e34a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy: 0.5, loss: 0.6935068517923355\n",
      "epoch: 1, accuracy: 0.625, loss: 0.6809237599372864\n",
      "epoch: 2, accuracy: 0.625, loss: 0.6781993582844734\n",
      "epoch: 3, accuracy: 0.625, loss: 0.6737440377473831\n",
      "epoch: 4, accuracy: 0.625, loss: 0.6708152890205383\n",
      "epoch: 5, accuracy: 0.875, loss: 0.6575960889458656\n",
      "epoch: 6, accuracy: 0.625, loss: 0.6570715457201004\n",
      "epoch: 7, accuracy: 0.875, loss: 0.6261524856090546\n",
      "epoch: 8, accuracy: 0.75, loss: 0.604952160269022\n",
      "epoch: 9, accuracy: 0.875, loss: 0.5835750326514244\n",
      "epoch: 10, accuracy: 0.875, loss: 0.5647474899888039\n",
      "epoch: 11, accuracy: 0.875, loss: 0.5461789630353451\n",
      "epoch: 12, accuracy: 1.0, loss: 0.5236970223486423\n",
      "testing accuracy: 1.0 and loss: 0.6628731191158295\n",
      "epoch: 0, accuracy: 0.5, loss: 0.7105482034385204\n",
      "epoch: 1, accuracy: 0.5, loss: 0.6842015944421291\n",
      "epoch: 2, accuracy: 0.5, loss: 0.657867893576622\n",
      "epoch: 3, accuracy: 0.5, loss: 0.6216005813330412\n",
      "epoch: 4, accuracy: 0.5, loss: 0.5865693334490061\n",
      "epoch: 5, accuracy: 0.5, loss: 0.5500349495559931\n",
      "epoch: 6, accuracy: 0.75, loss: 0.5024227118119597\n",
      "epoch: 7, accuracy: 0.75, loss: 0.459898947738111\n",
      "epoch: 8, accuracy: 0.875, loss: 0.42798649333417416\n",
      "epoch: 9, accuracy: 0.875, loss: 0.4076785552315414\n",
      "epoch: 10, accuracy: 0.875, loss: 0.3937033526599407\n",
      "epoch: 11, accuracy: 0.875, loss: 0.3836020501330495\n",
      "epoch: 12, accuracy: 1.0, loss: 0.37660723901353776\n",
      "testing accuracy: 1.0 and loss: 0.6628731191158295\n",
      "epoch: 0, accuracy: 0.25, loss: 0.7001875936985016\n",
      "epoch: 1, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 2, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 3, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 4, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 5, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 6, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 7, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 8, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 9, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 10, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 11, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 12, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 13, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 14, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 15, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 16, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 17, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 18, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 19, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 20, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 21, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 22, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 23, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 24, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 25, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 26, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 27, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 28, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 29, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 30, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 31, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 32, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 33, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 34, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 35, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 36, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 37, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 38, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 39, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 40, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 41, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 42, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 43, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 44, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 45, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 46, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 47, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 48, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 49, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 50, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 51, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 52, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 53, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 54, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 55, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 56, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 57, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 58, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 59, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 60, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 61, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 62, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 63, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 64, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 65, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 66, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 67, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 68, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 69, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 70, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 71, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 72, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 73, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 74, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 75, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 76, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 77, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 78, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 79, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 80, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 81, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 82, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 83, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 84, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 85, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 86, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 87, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 88, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 89, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 90, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 91, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 92, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 93, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 94, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 95, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 96, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 97, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 98, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 99, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 100, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 101, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 102, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 103, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 104, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 105, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 106, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 107, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 108, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 109, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 110, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 111, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 112, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 113, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 114, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 115, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 116, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 117, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 118, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 119, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 120, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 121, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 122, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 123, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 124, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 125, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 126, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 127, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 128, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 129, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 130, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 131, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 132, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 133, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 134, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 135, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 136, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 137, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 138, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 139, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 140, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 141, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 142, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 143, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 144, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 145, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 146, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 147, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 148, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 149, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 150, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 151, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 152, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 153, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 154, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 155, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 156, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 157, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 158, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 159, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 160, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 161, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 162, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 163, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 164, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 165, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 166, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 167, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 168, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 169, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 170, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 171, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 172, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 173, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 174, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 175, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 176, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 177, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 178, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 179, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 180, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 181, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 182, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 183, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 184, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 185, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 186, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 187, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 188, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 189, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 190, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 191, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 192, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 193, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 194, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 195, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 196, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 197, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 198, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 199, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 200, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 201, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 202, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 203, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 204, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 205, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 206, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 207, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 208, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 209, accuracy: 0.5, loss: 0.6931471824645996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 210, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 211, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 212, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 213, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 214, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 215, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 216, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 217, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 218, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 219, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 220, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 221, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 222, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 223, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 224, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 225, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 226, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 227, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 228, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 229, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 230, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 231, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 232, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 233, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 234, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 235, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 236, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 237, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 238, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 239, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 240, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 241, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 242, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 243, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 244, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 245, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 246, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 247, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 248, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 249, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 250, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 251, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 252, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 253, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 254, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 255, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 256, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 257, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 258, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 259, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 260, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 261, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 262, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 263, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 264, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 265, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 266, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 267, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 268, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 269, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 270, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 271, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 272, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 273, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 274, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 275, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 276, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 277, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 278, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 279, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 280, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 281, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 282, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 283, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 284, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 285, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 286, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 287, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 288, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 289, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 290, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 291, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 292, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 293, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 294, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 295, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 296, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 297, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 298, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 299, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 300, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 301, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 302, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 303, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 304, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 305, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 306, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 307, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 308, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 309, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 310, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 311, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 312, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 313, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 314, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 315, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 316, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 317, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 318, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 319, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 320, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 321, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 322, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 323, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 324, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 325, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 326, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 327, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 328, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 329, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 330, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 331, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 332, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 333, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 334, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 335, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 336, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 337, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 338, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 339, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 340, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 341, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 342, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 343, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 344, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 345, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 346, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 347, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 348, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 349, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 350, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 351, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 352, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 353, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 354, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 355, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 356, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 357, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 358, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 359, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 360, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 361, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 362, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 363, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 364, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 365, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 366, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 367, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 368, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 369, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 370, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 371, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 372, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 373, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 374, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 375, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 376, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 377, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 378, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 379, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 380, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 381, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 382, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 383, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 384, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 385, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 386, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 387, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 388, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 389, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 390, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 391, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 392, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 393, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 394, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 395, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 396, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 397, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 398, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 399, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 400, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 401, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 402, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 403, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 404, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 405, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 406, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 407, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 408, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 409, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 410, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 411, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 412, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 413, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 414, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 415, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 416, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 417, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 418, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 419, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 420, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 421, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 422, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 423, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 424, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 425, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 426, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 427, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 428, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 429, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 430, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 431, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 432, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 433, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 434, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 435, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 436, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 437, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 438, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 439, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 440, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 441, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 442, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 443, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 444, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 445, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 446, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 447, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 448, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 449, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 450, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 451, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 452, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 453, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 454, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 455, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 456, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 457, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 458, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 459, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 460, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 461, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 462, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 463, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 464, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 465, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 466, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 467, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 468, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 469, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 470, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 471, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 472, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 473, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 474, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 475, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 476, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 477, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 478, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 479, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 480, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 481, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 482, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 483, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 484, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 485, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 486, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 487, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 488, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 489, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 490, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 491, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 492, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 493, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 494, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 495, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 496, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 497, accuracy: 0.5, loss: 0.6931471824645996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 498, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 499, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 500, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 501, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 502, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 503, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 504, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 505, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 506, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 507, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 508, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 509, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 510, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 511, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 512, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 513, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 514, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 515, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 516, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 517, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 518, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 519, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 520, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 521, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 522, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 523, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 524, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 525, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 526, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 527, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 528, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 529, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 530, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 531, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 532, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 533, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 534, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 535, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 536, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 537, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 538, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 539, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 540, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 541, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 542, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 543, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 544, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 545, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 546, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 547, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 548, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 549, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 550, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 551, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 552, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 553, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 554, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 555, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 556, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 557, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 558, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 559, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 560, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 561, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 562, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 563, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 564, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 565, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 566, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 567, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 568, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 569, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 570, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 571, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 572, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 573, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 574, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 575, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 576, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 577, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 578, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 579, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 580, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 581, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 582, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 583, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 584, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 585, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 586, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 587, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 588, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 589, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 590, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 591, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 592, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 593, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 594, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 595, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 596, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 597, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 598, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 599, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 600, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 601, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 602, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 603, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 604, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 605, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 606, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 607, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 608, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 609, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 610, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 611, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 612, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 613, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 614, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 615, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 616, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 617, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 618, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 619, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 620, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 621, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 622, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 623, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 624, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 625, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 626, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 627, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 628, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 629, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 630, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 631, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 632, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 633, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 634, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 635, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 636, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 637, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 638, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 639, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 640, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 641, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 642, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 643, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 644, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 645, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 646, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 647, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 648, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 649, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 650, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 651, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 652, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 653, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 654, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 655, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 656, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 657, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 658, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 659, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 660, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 661, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 662, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 663, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 664, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 665, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 666, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 667, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 668, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 669, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 670, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 671, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 672, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 673, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 674, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 675, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 676, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 677, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 678, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 679, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 680, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 681, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 682, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 683, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 684, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 685, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 686, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 687, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 688, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 689, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 690, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 691, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 692, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 693, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 694, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 695, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 696, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 697, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 698, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 699, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 700, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 701, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 702, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 703, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 704, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 705, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 706, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 707, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 708, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 709, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 710, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 711, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 712, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 713, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 714, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 715, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 716, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 717, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 718, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 719, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 720, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 721, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 722, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 723, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 724, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 725, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 726, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 727, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 728, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 729, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 730, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 731, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 732, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 733, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 734, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 735, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 736, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 737, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 738, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 739, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 740, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 741, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 742, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 743, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 744, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 745, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 746, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 747, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 748, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 749, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 750, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 751, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 752, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 753, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 754, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 755, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 756, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 757, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 758, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 759, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 760, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 761, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 762, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 763, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 764, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 765, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 766, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 767, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 768, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 769, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 770, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 771, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 772, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 773, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 774, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 775, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 776, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 777, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 778, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 779, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 780, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 781, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 782, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 783, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 784, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 785, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 786, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 787, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 788, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 789, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 790, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 791, accuracy: 0.5, loss: 0.6931471824645996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 792, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 793, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 794, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 795, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 796, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 797, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 798, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 799, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 800, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 801, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 802, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 803, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 804, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 805, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 806, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 807, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 808, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 809, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 810, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 811, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 812, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 813, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 814, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 815, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 816, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 817, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 818, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 819, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 820, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 821, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 822, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 823, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 824, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 825, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 826, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 827, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 828, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 829, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 830, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 831, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 832, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 833, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 834, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 835, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 836, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 837, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 838, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 839, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 840, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 841, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 842, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 843, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 844, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 845, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 846, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 847, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 848, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 849, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 850, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 851, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 852, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 853, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 854, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 855, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 856, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 857, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 858, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 859, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 860, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 861, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 862, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 863, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 864, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 865, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 866, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 867, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 868, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 869, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 870, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 871, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 872, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 873, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 874, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 875, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 876, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 877, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 878, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 879, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 880, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 881, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 882, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 883, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 884, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 885, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 886, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 887, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 888, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 889, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 890, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 891, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 892, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 893, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 894, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 895, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 896, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 897, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 898, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 899, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 900, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 901, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 902, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 903, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 904, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 905, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 906, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 907, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 908, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 909, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 910, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 911, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 912, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 913, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 914, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 915, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 916, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 917, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 918, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 919, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 920, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 921, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 922, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 923, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 924, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 925, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 926, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 927, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 928, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 929, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 930, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 931, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 932, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 933, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 934, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 935, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 936, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 937, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 938, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 939, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 940, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 941, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 942, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 943, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 944, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 945, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 946, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 947, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 948, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 949, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 950, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 951, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 952, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 953, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 954, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 955, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 956, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 957, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 958, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 959, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 960, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 961, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 962, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 963, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 964, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 965, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 966, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 967, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 968, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 969, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 970, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 971, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 972, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 973, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 974, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 975, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 976, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 977, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 978, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 979, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 980, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 981, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 982, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 983, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 984, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 985, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 986, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 987, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 988, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 989, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 990, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 991, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 992, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 993, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 994, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 995, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 996, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 997, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 998, accuracy: 0.5, loss: 0.6931471824645996\n",
      "epoch: 999, accuracy: 0.5, loss: 0.6931471824645996\n",
      "testing accuracy: 1.0 and loss: 0.6628731191158295\n"
     ]
    }
   ],
   "source": [
    "embedding_sizes = [10, 100, 1000]\n",
    "smt_times = []\n",
    "backpropagation_times = []\n",
    "\n",
    "for embedding_size in embedding_sizes:\n",
    "\n",
    "    model = Model(embedding_size)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "    model.to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_backpropagation(model, points, labels, criterion, optimizer, num_epochs)\n",
    "    backpropagation_times.append(time.time() - start_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_smt(model, points, labels)\n",
    "    smt_times.append(time.time() - start_time)\n",
    "    test(model, points, labels, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31007e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560c76c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
